{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rlawogjs96/deeplearningnlp_starters/blob/main/GloVe.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PI7_yAulkCf2"
      },
      "source": [
        "#### 1. What is GloVe?\n",
        "\n",
        "카운트 기반과 예측 기반을 모두 사용하는 단어 임베딩 방법론. Word2Vec 과 같이 뛰어난 성능을 보여준다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vzRCQcVLj8X0",
        "outputId": "133b7946-1272-4b95-e03d-216b50c23589"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting glove_python_binary\n",
            "  Downloading glove_python_binary-0.2.0-cp37-cp37m-manylinux1_x86_64.whl (948 kB)\n",
            "\u001b[K     |████████████████████████████████| 948 kB 5.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from glove_python_binary) (1.7.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from glove_python_binary) (1.21.6)\n",
            "Installing collected packages: glove-python-binary\n",
            "Successfully installed glove-python-binary-0.2.0\n"
          ]
        }
      ],
      "source": [
        "pip install glove_python_binary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a0kcln89rvym",
        "outputId": "2835390f-26f0-41a4-d45f-32a9dd48bacc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "glove-python-binary           0.2.0\n"
          ]
        }
      ],
      "source": [
        "pip list | grep glove"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h85tAlvyr6qe",
        "outputId": "93cb3025-3923-4a0f-c834-471f55ae026a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "UxuemuepsApX"
      },
      "outputs": [],
      "source": [
        "import urllib.request \n",
        "import zipfile \n",
        "from lxml import etree \n",
        "import re \n",
        "from nltk.tokenize import word_tokenize, sent_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BQFusTvVsRTj",
        "outputId": "d7527c15-de30-4dd8-b817-7afc39a84a4b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('ted_en-20160408.xml', <http.client.HTTPMessage at 0x7fccef1a2d50>)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "# 데이터 다운로드\n",
        "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/GaoleMeng/RNN-and-FFNN-textClassification/master/ted_en-20160408.xml\", filename=\"ted_en-20160408.xml\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "QxX7Rn_-sdLW"
      },
      "outputs": [],
      "source": [
        "targetXML = open('ted_en-20160408.xml', 'r', encoding='UTF8')\n",
        "target_text = etree.parse(targetXML)\n",
        "\n",
        "# xml 파일로부터 <content>와 </content> 사이의 내용만 가져옴.\n",
        "parse_text = '\\n'.join(target_text.xpath('//content/text()'))\n",
        "\n",
        "# 정규 표현식의 sub 모듈을 통해 content 중간에 등장하는 (Audio), (Laughter) 등의 배경음 부분을 제거.\n",
        "# 해당 코드는 괄호로 구성된 내용을 제거.\n",
        "content_text = re.sub(r'\\([^)]*\\)', '', parse_text)\n",
        "\n",
        "# 입력 코퍼스에 대해서 NLTK를 이용하여 문장 토큰화를 수행.\n",
        "sent_text = sent_tokenize(content_text)\n",
        "\n",
        "# 각 문장에 대해서 구두점을 제거하고, 대문자를 소문자로 변환.\n",
        "normalized_text = []\n",
        "for string in sent_text:\n",
        "     tokens = re.sub(r\"[^a-z0-9]+\", \" \", string.lower())\n",
        "     normalized_text.append(tokens)\n",
        "\n",
        "# 각 문장에 대해서 NLTK를 이용하여 단어 토큰화를 수행.\n",
        "result = [word_tokenize(sentence) for sentence in normalized_text] "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "eotZArb8shac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a233d359-c11a-4b30-a84e-b934fb112a49"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "총 샘플의 개수 : 273380\n"
          ]
        }
      ],
      "source": [
        "print('총 샘플의 개수 : {}'.format(len(result)))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from glove import Corpus, Glove\n",
        "\n",
        "corpus = Corpus() \n",
        "\n",
        "# 훈련 데이터로부터 GloVe에서 사용할 동시 등장 행렬 생성\n",
        "corpus.fit(result, window=5)\n",
        "glove = Glove(no_components=100, learning_rate=0.05)\n",
        "\n",
        "# 학습에 이용할 쓰레드의 개수는 4로 설정, 에포크는 20.\n",
        "glove.fit(corpus.matrix, epochs=20, no_threads=4, verbose=True)\n",
        "glove.add_dictionary(corpus.dictionary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ie5vQUoSpPyx",
        "outputId": "504d2981-24b7-4d94-afaa-d78cd883d55d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performing 20 training epochs with 4 threads\n",
            "Epoch 0\n",
            "Epoch 1\n",
            "Epoch 2\n",
            "Epoch 3\n",
            "Epoch 4\n",
            "Epoch 5\n",
            "Epoch 6\n",
            "Epoch 7\n",
            "Epoch 8\n",
            "Epoch 9\n",
            "Epoch 10\n",
            "Epoch 11\n",
            "Epoch 12\n",
            "Epoch 13\n",
            "Epoch 14\n",
            "Epoch 15\n",
            "Epoch 16\n",
            "Epoch 17\n",
            "Epoch 18\n",
            "Epoch 19\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "glove.most_similar()는 입력 단어의 가장 유사한 단어들의 리스트를 리턴한다."
      ],
      "metadata": {
        "id": "4K9my5F3GS3c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(glove.most_similar(\"man\"))\n",
        "print(glove.most_similar(\"boy\"))\n",
        "print(glove.most_similar(\"water\"))\n",
        "print(glove.most_similar(\"physics\"))\n",
        "print(glove.most_similar(\"muscle\"))\n",
        "print(glove.most_similar(\"clean\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ihERU8J9FTf8",
        "outputId": "a44c372b-04a1-4a21-f10e-f4d426edea68"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('woman', 0.9595697346515466), ('guy', 0.8837675555576077), ('girl', 0.8495041063180306), ('young', 0.8411860951269678)]\n",
            "[('girl', 0.941511332305726), ('kid', 0.8285635486579125), ('woman', 0.825393266629894), ('man', 0.8217067921136911)]\n",
            "[('fresh', 0.842281969254099), ('air', 0.8370272573269945), ('clean', 0.8216793282506704), ('electricity', 0.8144958679551221)]\n",
            "[('chemistry', 0.8866871965443668), ('beauty', 0.878268287319308), ('economics', 0.8615575207744331), ('mathematics', 0.8507950597739462)]\n",
            "[('tissue', 0.8367373598904942), ('nerve', 0.8214273364349003), ('skeletal', 0.7835144234788932), ('channel', 0.7523279257474466)]\n",
            "[('water', 0.8216793282506706), ('fresh', 0.8010357861113125), ('wind', 0.7991583942969046), ('heat', 0.7961808789239007)]\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyND5IDOHMx8+cVX8cof71hz",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}